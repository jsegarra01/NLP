{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "sequence_length = 10\n",
    "nhead = 4\n",
    "num_encoder_layers = 2\n",
    "dim_feedforward = 512\n",
    "file_path = 'C:\\\\Users\\\\josep\\\\Downloads\\\\LTR.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, sequence_length):\n",
    "    \"\"\"Preprocess the text into sequences of tokens.\"\"\"\n",
    "    # Tokenize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    for i in range(len(tokens) - sequence_length):\n",
    "        seq = tokens[i:i + sequence_length + 1]\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "def build_vocab(sequences):\n",
    "    \"\"\"Build a vocabulary from the sequences.\"\"\"\n",
    "    all_tokens = [token for seq in sequences for token in seq]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(token_counts.items(), 1)}\n",
    "    vocab['<PAD>'] = 0  # Add padding token\n",
    "    return vocab\n",
    "\n",
    "def sequences_to_indices(sequences, vocab):\n",
    "    \"\"\"Convert sequences of tokens to sequences of indices.\"\"\"\n",
    "    return [[vocab[token] for token in seq] for seq in sequences]\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"Read text from a file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the text file\n",
    "text = read_text_file(file_path)\n",
    "sequences = preprocess_text(text, sequence_length)\n",
    "vocab = build_vocab(sequences)\n",
    "indexed_sequences = sequences_to_indices(sequences, vocab)\n",
    "\n",
    "# Adjust vocab_size according to the actual vocabulary size\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = LanguageDataset(indexed_sequences)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        \n",
    "        super(PerceptronLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        batch_size, seq_len, _ = embedded.shape\n",
    "        embedded = embedded.view(batch_size * seq_len, -1)  # Flatten the sequence dimension\n",
    "        out = torch.relu(self.fc1(embedded))\n",
    "        out = self.fc2(out)\n",
    "        return out.view(batch_size, seq_len, -1)  # Reshape to (batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_perceptron(model, data_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    model.to(device)\n",
    "    total_training_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].flatten()  # Flatten targets for loss calculation\n",
    "            \n",
    "            outputs = model(inputs).reshape(-1, vocab_size)  # Reshape outputs for loss calculation\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s')\n",
    "    return total_training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_perceptron(model, start_sequence, vocab, reverse_vocab, max_length=20):\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Convert the start_sequence to indices\n",
    "    sequence = [vocab[word] for word in start_sequence]\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_length - len(start_sequence)):\n",
    "        inputs = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        # Get the last token probabilities\n",
    "        last_token_logits = outputs[0, -1, :]\n",
    "        next_token_probs = F.softmax(last_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample the next token\n",
    "        next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "        \n",
    "        # Add the token to the sequence\n",
    "        sequence.append(next_token)\n",
    "        \n",
    "        # Stop if the end token is generated (optional)\n",
    "        if reverse_vocab[next_token] == '<END>':\n",
    "            break\n",
    "\n",
    "    # Convert indices back to words\n",
    "    generated_sequence = [reverse_vocab[idx] for idx in sequence]\n",
    "    \n",
    "    return ' '.join(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"Elman RNN language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the language model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of the embeddings.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            num_layers (int): Number of recurrent layers.\n",
    "        \"\"\"\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "            hidden (torch.Tensor): Hidden state for RNN.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, sequence_length, vocab_size).\n",
    "            torch.Tensor: Updated hidden state.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.fc(rnn_out)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Batch size.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Initial hidden state.\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.rnn.num_layers, batch_size, self.rnn.hidden_size).zero_()\n",
    "        return hidden\n",
    "\n",
    "\n",
    "\n",
    "def train_model_RNN(model, data_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    model.to(device)\n",
    "    total_training_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)  # Flatten targets for loss calculation\n",
    "            \n",
    "            hidden = model.init_hidden(inputs.size(0))  # Initialize hidden state\n",
    "            hidden = hidden.to(device)\n",
    "            \n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            outputs = outputs.contiguous().view(-1, vocab_size)  # Reshape outputs for loss calculation\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return total_training_time\n",
    "\n",
    "def generate_sentence_RNN(model, start_sequence, vocab, reverse_vocab, max_length=20):\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Convert the start_sequence to indices\n",
    "    sequence = [vocab[word] for word in start_sequence]\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_length - len(start_sequence)):\n",
    "        inputs = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Get the last token probabilities\n",
    "        last_token_logits = outputs[0, -1, :]\n",
    "        next_token_probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample the next token\n",
    "        next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "        \n",
    "        # Add the token to the sequence\n",
    "        sequence.append(next_token)\n",
    "        \n",
    "        # Stop if the end token is generated (optional)\n",
    "        if reverse_vocab[next_token] == '<END>':\n",
    "            break\n",
    "\n",
    "    # Convert indices back to words\n",
    "    generated_sequence = [reverse_vocab[idx] for idx in sequence]\n",
    "    \n",
    "    return ' '.join(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)  # Transformer expects (sequence_length, batch_size, embedding_dim)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x.transpose(0, 1)  # Revert to (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "# Transformer Language Model class\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, nhead, num_encoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_seq_length)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.transpose(0, 1)  # Transformer expects (sequence_length, batch_size, embedding_dim)\n",
    "        output = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = output.transpose(0, 1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Training function\n",
    "def train_model_transformer(model, data_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    total_training_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            src_key_padding_mask = (inputs == 0)  # Correct mask shape (batch_size, sequence_length)\n",
    "            outputs = model(inputs, src_key_padding_mask=src_key_padding_mask)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "    return total_training_time\n",
    "\n",
    "\n",
    "# Generate sentence function\n",
    "def generate_sentence_transformer(model, start_sequence, vocab, reverse_vocab, max_length=20):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    sequence = [vocab[word] for word in start_sequence]\n",
    "    input_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "    generated_sequence = sequence.copy()\n",
    "    \n",
    "    for _ in range(max_length - len(start_sequence)):\n",
    "        src_key_padding_mask = (input_tensor == 0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        next_token_logits = outputs[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "        \n",
    "        if next_token == vocab['<PAD>']:\n",
    "            break\n",
    "        \n",
    "        generated_sequence.append(next_token)\n",
    "        input_tensor = torch.tensor([generated_sequence[-11:]], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated_words = [reverse_vocab[idx] for idx in generated_sequence]\n",
    "    return ' '.join(generated_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            if isinstance(model, (RNNLanguageModel)):\n",
    "                hidden = model.init_hidden(inputs.size(0)).to(device)\n",
    "                outputs, hidden = model(inputs, hidden)\n",
    "                outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "            elif isinstance(model, (PerceptronLanguageModel)):\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "            else:\n",
    "                src_key_padding_mask = (inputs == 0)  # Assumes 0 is the padding index\n",
    "                outputs = model(inputs, src_key_padding_mask=src_key_padding_mask)\n",
    "                outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            total_tokens += targets.size(0)\n",
    "\n",
    "        average_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron model...\n",
      "Epoch 1, Average Loss: 5.0603, Time: 48.55s\n",
      "Epoch 2, Average Loss: 4.6168, Time: 50.75s\n",
      "Epoch 3, Average Loss: 4.5062, Time: 50.35s\n",
      "Epoch 4, Average Loss: 4.4577, Time: 49.20s\n",
      "Epoch 5, Average Loss: 4.4326, Time: 51.67s\n",
      "Perceptron Model - Training Time: 250.51s, Perplexity: 81.18\n",
      "Training RNN model...\n",
      "Epoch 1, Average Loss: 4.8669\n",
      "Epoch 2, Average Loss: 3.9678\n",
      "Epoch 3, Average Loss: 3.6253\n",
      "Epoch 4, Average Loss: 3.4213\n",
      "Epoch 5, Average Loss: 3.2835\n",
      "RNN Model - Training Time: 307.96s, Perplexity: 23.52\n",
      "Training Transformer model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 3.5218\n",
      "Epoch 2, Average Loss: 2.3410\n",
      "Epoch 3, Average Loss: 1.5958\n",
      "Epoch 4, Average Loss: 1.1692\n",
      "Epoch 5, Average Loss: 0.9625\n",
      "RNN Model - Training Time: 720.08s, Perplexity: 1.84\n"
     ]
    }
   ],
   "source": [
    "# Determine the device to be used (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Init models\n",
    "perceptron_model = PerceptronLanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "rnn_model = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "transformer_model = TransformerLanguageModel(vocab_size, embedding_dim, nhead, num_encoder_layers, dim_feedforward, sequence_length + 1)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "perceptron_optimizer = optim.Adam(perceptron_model.parameters(), lr=learning_rate)\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "transformer_optimizer = optim.Adam(transformer_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train models\n",
    "print(\"Training perceptron model...\")\n",
    "perceptron_training_time = train_model_perceptron(perceptron_model, data_loader, criterion, perceptron_optimizer, num_epochs, device)\n",
    "perceptron_perplexity = calculate_perplexity(perceptron_model, data_loader, criterion, device)\n",
    "print(f\"Perceptron Model - Training Time: {perceptron_training_time:.2f}s, Perplexity: {perceptron_perplexity:.2f}\")\n",
    "\n",
    "print(\"Training RNN model...\")\n",
    "rnn_training_time = train_model_RNN(rnn_model, data_loader, criterion, rnn_optimizer, num_epochs, device)\n",
    "rnn_perplexity = calculate_perplexity(rnn_model, data_loader, criterion, device)\n",
    "print(f\"RNN Model - Training Time: {rnn_training_time:.2f}s, Perplexity: {rnn_perplexity:.2f}\")\n",
    "\n",
    "print(\"Training Transformer model...\")\n",
    "trans_training_time = train_model_transformer(transformer_model, data_loader, criterion, transformer_optimizer, num_epochs, device)\n",
    "trans_perplexity = calculate_perplexity(transformer_model, data_loader, criterion, device)\n",
    "print(f\"RNN Model - Training Time: {trans_training_time:.2f}s, Perplexity: {trans_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hobbit went down at the gaffer did this hour after all of the great so i wish to welcome\n",
      "the hobbit went forward on their way swept up and turned from gandalf forgive them his eyes and the fire\n",
      "the hobbit went went went went went went went went the bad went straight went on riding on from the\n"
     ]
    }
   ],
   "source": [
    "start_sequence = ['the', 'hobbit', 'went']\n",
    "\n",
    "generated_sentence_perceptron = generate_sentence_perceptron(perceptron_model, start_sequence, vocab, {v: k for k, v in vocab.items()})\n",
    "print(generated_sentence_perceptron)\n",
    "\n",
    "generated_sentence_RNN = generate_sentence_RNN(rnn_model, start_sequence, vocab, {v: k for k, v in vocab.items()})\n",
    "print(generated_sentence_RNN)\n",
    "\n",
    "generated_sentence = generate_sentence_transformer(transformer_model, start_sequence, vocab, {v: k for k, v in vocab.items()})\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTrained Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1493e154ba4bd68355eb80ebe571ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.8214, 'train_samples_per_second': 3.652, 'train_steps_per_second': 3.652, 'train_loss': 2.997549374898275, 'epoch': 3.0}\n",
      "The Hobbit went on to be the first to-bearer of the Fellowship of the Ring, and the first to be the first to-bearer of the Fellowship of the Ring.\n",
      "\n",
      "The Fellowship of the Ring was the first to be the first to be the first to-be-be-first to-be-first to-be-first to-be-be-be-be-first to-be-be-first to-be-first to-be-first\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Cargar el archivo de texto\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Inicializar el tokenizador y el modelo\n",
    "tokenizerPre = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "modelPre = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizerPre(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Crear etiquetas (labels) que son los mismos input_ids\n",
    "labels = input_ids.clone()\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.input_ids[idx], 'labels': self.labels[idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "# Crear el conjunto de datos\n",
    "dataset = CustomDataset(input_ids, labels)\n",
    "\n",
    "# Configurar los argumentos del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Inicializar el entrenador\n",
    "trainer = Trainer(\n",
    "    model=modelPre,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Generar texto\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "modelPre.to(device)\n",
    "\n",
    "prompt = \"The Hobbit went\"\n",
    "inputs = tokenizerPre(prompt, return_tensors='pt')\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "outputs = modelPre.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(tokenizerPre.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers, decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "sequence_length = 10\n",
    "nhead = 4\n",
    "num_encoder_layers = 2\n",
    "dim_feedforward = 512\n",
    "file_path = 'LTR.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    \"\"\"Read text from a file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "text = read_text_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standar Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def st_preprocess_text(text, sequence_length):\n",
    "    \"\"\"Preprocess the text into sequences of tokens.\"\"\"\n",
    "    # Tokenize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    for i in range(len(tokens) - sequence_length):\n",
    "        seq = tokens[i:i + sequence_length + 1]\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "def st_build_vocab(sequences):\n",
    "    \"\"\"Build a vocabulary from the sequences.\"\"\"\n",
    "    all_tokens = [token for seq in sequences for token in seq]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(token_counts.items(), 1)}\n",
    "    vocab['<PAD>'] = 0  # Add padding token\n",
    "    return vocab\n",
    "\n",
    "def st_sequences_to_indices(sequences, vocab):\n",
    "    \"\"\"Convert sequences of tokens to sequences of indices.\"\"\"\n",
    "    return [[vocab[token] for token in seq] for seq in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the text file\n",
    "st_sequences = st_preprocess_text(text, sequence_length)\n",
    "st_vocab = st_build_vocab(st_sequences)\n",
    "st_indexed_sequences = st_sequences_to_indices(st_sequences, st_vocab)\n",
    "\n",
    "# Adjust vocab_size according to the actual vocabulary size\n",
    "st_vocab_size = len(st_vocab)\n",
    "\n",
    "# Create the dataset and data loader\n",
    "st_dataset = LanguageDataset(st_indexed_sequences)\n",
    "st_data_loader = DataLoader(st_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_preprocess_text(text):\n",
    "    \"\"\"Preprocess the text into lines.\"\"\"\n",
    "    # Normalize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
    "    return text.split('\\n')\n",
    "\n",
    "# Preprocess text into sequences\n",
    "def bpe_preprocess_text_to_sequences(text, sequence_length, tokenizer):\n",
    "    \"\"\"Preprocess the text into sequences of tokens using BPE tokenization.\"\"\"\n",
    "    # Tokenize text using the BPE tokenizer\n",
    "    encoding = tokenizer.encode(text)\n",
    "    tokens = encoding.tokens\n",
    "\n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    for i in range(len(tokens) - sequence_length):\n",
    "        seq = tokens[i:i + sequence_length + 1]\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# Build vocabulary\n",
    "def bpe_build_vocab(tokenizer):\n",
    "    \"\"\"Build a vocabulary from the tokenizer.\"\"\"\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(vocab.items())}\n",
    "    return vocab\n",
    "\n",
    "# Convert sequences to indices\n",
    "def bpe_sequences_to_indices(sequences, vocab):\n",
    "    \"\"\"Convert sequences of tokens to sequences of indices.\"\"\"\n",
    "    return [[vocab.get(token, vocab['<UNK>']) for token in seq] for seq in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=[\"<PAD>\", \"<UNK>\", \"<END>\"])\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "\n",
    "# Read and preprocess the text file\n",
    "lines = bpe_preprocess_text(text)\n",
    "tokenizer.train_from_iterator(lines, trainer)\n",
    "bpe_sequences = bpe_preprocess_text_to_sequences(text, sequence_length, tokenizer)\n",
    "bpe_vocab = bpe_build_vocab(tokenizer)\n",
    "bpe_indexed_sequences = bpe_sequences_to_indices(bpe_sequences, bpe_vocab)\n",
    "\n",
    "bpe_vocab_size = len(bpe_vocab)\n",
    "\n",
    "# Create the dataset and data loader\n",
    "bpe_dataset = LanguageDataset(bpe_indexed_sequences)\n",
    "bpe_data_loader = DataLoader(bpe_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLanguageModel(nn.Module):\n",
    "    \"\"\"Feedforward language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initialize the language model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of the embeddings.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "        \"\"\"\n",
    "        super(PerceptronLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size * sequence_length, vocab_size).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        batch_size, seq_len, _ = embedded.shape\n",
    "        embedded = embedded.view(batch_size * seq_len, -1)  # Flatten the sequence dimension\n",
    "        out = torch.relu(self.fc1(embedded))\n",
    "        out = self.fc2(out)\n",
    "        return out.view(batch_size, seq_len, -1)  # Reshape to (batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_perceptron(model, data_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    model.to(device)\n",
    "    total_training_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].flatten()  # Flatten targets for loss calculation\n",
    "            \n",
    "            outputs = model(inputs).reshape(-1, st_vocab_size)  # Reshape outputs for loss calculation\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s')\n",
    "    return total_training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_perceptron(model, start_sequence, vocab, reverse_vocab, max_length=20):\n",
    "    \"\"\"\n",
    "    Generate a sentence using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model.\n",
    "        start_sequence (list of str): The initial sequence of words.\n",
    "        vocab (dict): The word to index mapping.\n",
    "        reverse_vocab (dict): The index to word mapping.\n",
    "        max_length (int): The maximum length of the generated sentence.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated sentence.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Convert the start_sequence to indices\n",
    "    sequence = [vocab[word] for word in start_sequence]\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_length - len(start_sequence)):\n",
    "        inputs = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        # Get the last token probabilities\n",
    "        last_token_logits = outputs[0, -1, :]\n",
    "        next_token_probs = F.softmax(last_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample the next token\n",
    "        next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "        \n",
    "        # Add the token to the sequence\n",
    "        sequence.append(next_token)\n",
    "        \n",
    "        # Stop if the end token is generated (optional)\n",
    "        if reverse_vocab[next_token] == '<END>':\n",
    "            break\n",
    "\n",
    "    # Convert indices back to words\n",
    "    generated_sequence = [reverse_vocab[idx] for idx in sequence]\n",
    "    \n",
    "    return ' '.join(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"Elman RNN language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the language model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of the embeddings.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            num_layers (int): Number of recurrent layers.\n",
    "        \"\"\"\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "            hidden (torch.Tensor): Hidden state for RNN.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, sequence_length, vocab_size).\n",
    "            torch.Tensor: Updated hidden state.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.fc(rnn_out)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Batch size.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Initial hidden state.\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.rnn.num_layers, batch_size, self.rnn.hidden_size).zero_()\n",
    "        return hidden\n",
    "\n",
    "\n",
    "\n",
    "def train_model_RNN(model, data_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    model.to(device)\n",
    "    total_training_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)  # Flatten targets for loss calculation\n",
    "            \n",
    "            hidden = model.init_hidden(inputs.size(0))  # Initialize hidden state\n",
    "            hidden = hidden.to(device)\n",
    "            \n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            outputs = outputs.contiguous().view(-1, st_vocab_size)  # Reshape outputs for loss calculation\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return total_training_time\n",
    "\n",
    "def generate_sentence_RNN(model, start_sequence, vocab, reverse_vocab, max_length=20):\n",
    "    \"\"\"\n",
    "    Generate a sentence using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model.\n",
    "        start_sequence (list of str): The initial sequence of words.\n",
    "        vocab (dict): The word to index mapping.\n",
    "        reverse_vocab (dict): The index to word mapping.\n",
    "        max_length (int): The maximum length of the generated sentence.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated sentence.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Convert the start_sequence to indices\n",
    "    sequence = [vocab[word] for word in start_sequence]\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_length - len(start_sequence)):\n",
    "        inputs = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Get the last token probabilities\n",
    "        last_token_logits = outputs[0, -1, :]\n",
    "        next_token_probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample the next token\n",
    "        next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "        \n",
    "        # Add the token to the sequence\n",
    "        sequence.append(next_token)\n",
    "        \n",
    "        # Stop if the end token is generated (optional)\n",
    "        if reverse_vocab[next_token] == '<END>':\n",
    "            break\n",
    "\n",
    "    # Convert indices back to words\n",
    "    generated_sequence = [reverse_vocab[idx] for idx in sequence]\n",
    "    \n",
    "    return ' '.join(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.fc(rnn_out)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.rnn.num_layers, batch_size, self.rnn.hidden_size).zero_(),\n",
    "                  weight.new(self.rnn.num_layers, batch_size, self.rnn.hidden_size).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_st_lstm(model, data_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    total_training_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)  # Flatten targets for loss calculation\n",
    "            \n",
    "            hidden = model.init_hidden(inputs.size(0))  # Initialize hidden state\n",
    "            hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "            \n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            outputs = outputs.contiguous().view(-1, st_vocab_size)  # Reshape outputs for loss calculation\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return total_training_time\n",
    "\n",
    "def generate_sentence_st_lstm(model, start_sequence, vocab, reverse_vocab, max_length=20):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Convert the start_sequence to indices\n",
    "    sequence = [vocab[word] for word in start_sequence]\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_length - len(start_sequence)):\n",
    "        inputs = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Get the last token probabilities\n",
    "        last_token_logits = outputs[0, -1, :]\n",
    "        next_token_probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample the next token\n",
    "        next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "        \n",
    "        # Add the token to the sequence\n",
    "        sequence.append(next_token)\n",
    "        \n",
    "        # Stop if the end token is generated (optional)\n",
    "        if reverse_vocab[next_token] == '<END>':\n",
    "            break\n",
    "\n",
    "    # Convert indices back to words\n",
    "    generated_sequence = [reverse_vocab[idx] for idx in sequence]\n",
    "    \n",
    "    return ' '.join(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_bpe_lstm(model, data_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    total_training_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)  # Flatten targets for loss calculation\n",
    "            \n",
    "            hidden = model.init_hidden(inputs.size(0))  # Initialize hidden state\n",
    "            hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "            \n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            outputs = outputs.contiguous().view(-1, bpe_vocab_size)  # Reshape outputs for loss calculation\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return total_training_time\n",
    "\n",
    "def generate_sentence_bpe_lstm(model, start_sequence, tokenizer, vocab, max_length=20):\n",
    "    \"\"\"\n",
    "    Generate a sentence using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model.\n",
    "        start_sequence (list of str): The initial sequence of words.\n",
    "        tokenizer (Tokenizer): The BPE tokenizer.\n",
    "        vocab (dict): The word to index mapping.\n",
    "        max_length (int): The maximum length of the generated sentence.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated sentence.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the start_sequence\n",
    "    start_tokens = tokenizer.encode(' '.join(start_sequence)).ids\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    if isinstance(hidden, tuple):\n",
    "        hidden = tuple(h.to(device) for h in hidden)\n",
    "    else:\n",
    "        hidden = hidden.to(device)\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    generated_tokens = start_tokens.copy()\n",
    "    for _ in range(max_length - len(start_tokens)):\n",
    "        inputs = torch.tensor([generated_tokens], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Get the last token probabilities\n",
    "        last_token_logits = outputs[0, -1, :]\n",
    "        next_token_probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample the next token\n",
    "        next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "        \n",
    "        # Add the token to the sequence\n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Stop if the end token is generated (optional)\n",
    "        if next_token == vocab.get('<END>'):\n",
    "            break\n",
    "\n",
    "    # Decode the generated tokens to get the final sentence\n",
    "    decoded_tokens = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Manually add spaces where appropriate\n",
    "    sentence_with_spaces = ''\n",
    "    for token in tokenizer.encode(decoded_tokens).tokens:\n",
    "        if not token.startswith('##') and len(sentence_with_spaces) > 0:\n",
    "            sentence_with_spaces += ' '\n",
    "        sentence_with_spaces += token.replace('##', '')\n",
    "    \n",
    "    return sentence_with_spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(GRULanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.fc(rnn_out)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.rnn.num_layers, batch_size, self.rnn.hidden_size).zero_()\n",
    "        return hidden\n",
    "\n",
    "def pad_sequences(sequences, max_length):\n",
    "    return [seq + [0] * (max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "def train_model_gru(model, data_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    total_training_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            hidden = hidden.to(device)\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            outputs = outputs.contiguous().view(-1, st_vocab_size)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return total_training_time\n",
    "\n",
    "def generate_sentence_gru(model, start_sequence, vocab, reverse_vocab, max_length=20):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    sequence = [vocab[word] for word in start_sequence]\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    for _ in range(max_length - len(start_sequence)):\n",
    "        inputs = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "        last_token_logits = outputs[0, -1, :]\n",
    "        next_token_probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "        sequence.append(next_token)\n",
    "        if reverse_vocab[next_token] == '<END>':\n",
    "            break\n",
    "\n",
    "    generated_sequence = [reverse_vocab[idx] for idx in sequence]\n",
    "    return ' '.join(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calidad papi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            if isinstance(model, RNNLanguageModel):\n",
    "                hidden = model.init_hidden(inputs.size(0)).to(device)\n",
    "                outputs, hidden = model(inputs, hidden)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "            outputs = outputs.contiguous().view(-1, st_vocab_size)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            total_tokens += targets.size(0)\n",
    "\n",
    "    average_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, data_loader, criterion, device, vocab_size):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    " \n",
    "            if isinstance(model, (RNNLanguageModel, LSTMLanguageModel, GRULanguageModel)) or type(model).__name__ == 'GRULanguageModel':\n",
    "                hidden = model.init_hidden(inputs.size(0))\n",
    "                if isinstance(model, LSTMLanguageModel) or isinstance(model, GRULanguageModel):\n",
    "                    hidden = (hidden[0].to(device), hidden[1].to(device)) if isinstance(hidden, tuple) else hidden.to(device)\n",
    "                else:\n",
    "                    hidden = hidden.to(device)\n",
    "                outputs, hidden = model(inputs, hidden)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "            outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            total_tokens += targets.size(0)\n",
    "\n",
    "    average_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training perceptron model...\n",
      "Epoch 1, Average Loss: 5.0584, Time: 15.46s\n",
      "Epoch 2, Average Loss: 4.6170, Time: 15.43s\n",
      "Epoch 3, Average Loss: 4.5067, Time: 15.39s\n",
      "Epoch 4, Average Loss: 4.4581, Time: 15.35s\n",
      "Epoch 5, Average Loss: 4.4327, Time: 15.31s\n",
      "Epoch 6, Average Loss: 4.4169, Time: 15.13s\n",
      "Epoch 7, Average Loss: 4.4064, Time: 15.20s\n",
      "Epoch 8, Average Loss: 4.3985, Time: 15.21s\n",
      "Epoch 9, Average Loss: 4.3927, Time: 15.29s\n",
      "Epoch 10, Average Loss: 4.3880, Time: 15.22s\n",
      "Perceptron Model - Training Time: 152.99s, Perplexity: 78.22\n",
      "Training RNN model...\n",
      "Epoch 1, Average Loss: 4.8614\n",
      "Epoch 2, Average Loss: 3.9672\n",
      "Epoch 3, Average Loss: 3.6250\n",
      "Epoch 4, Average Loss: 3.4204\n",
      "Epoch 5, Average Loss: 3.2819\n",
      "Epoch 6, Average Loss: 3.1819\n",
      "Epoch 7, Average Loss: 3.1054\n",
      "Epoch 8, Average Loss: 3.0458\n",
      "Epoch 9, Average Loss: 2.9972\n",
      "Epoch 10, Average Loss: 2.9574\n",
      "RNN Model - Training Time: 172.80s, Perplexity: 17.64\n",
      "Training LSTM model...\n",
      "Epoch 1, Average Loss: 4.9459\n",
      "Epoch 2, Average Loss: 3.9084\n",
      "Epoch 3, Average Loss: 3.4413\n",
      "Epoch 4, Average Loss: 3.1601\n",
      "Epoch 5, Average Loss: 2.9724\n",
      "Epoch 6, Average Loss: 2.8386\n",
      "Epoch 7, Average Loss: 2.7383\n",
      "Epoch 8, Average Loss: 2.6598\n",
      "Epoch 9, Average Loss: 2.5963\n",
      "Epoch 10, Average Loss: 2.5440\n",
      "LSTM Model - Training Time: 185.87s, Perplexity: 11.54\n",
      "Training LSTM model with BPE Tokenizer...\n",
      "Epoch 1, Average Loss: 4.9371\n",
      "Epoch 2, Average Loss: 3.9768\n",
      "Epoch 3, Average Loss: 3.5405\n",
      "Epoch 4, Average Loss: 3.2760\n",
      "Epoch 5, Average Loss: 3.0984\n",
      "Epoch 6, Average Loss: 2.9711\n",
      "Epoch 7, Average Loss: 2.8752\n",
      "Epoch 8, Average Loss: 2.7997\n",
      "Epoch 9, Average Loss: 2.7393\n",
      "Epoch 10, Average Loss: 2.6888\n",
      "LSTM Model BPE Tokenizer - Training Time: 156.27s, Perplexity: 13.37\n",
      "Training GRU model...\n",
      "Epoch 1, Average Loss: 4.8212\n",
      "Epoch 2, Average Loss: 3.7560\n",
      "Epoch 3, Average Loss: 3.3665\n",
      "Epoch 4, Average Loss: 3.1480\n",
      "Epoch 5, Average Loss: 3.0052\n",
      "Epoch 6, Average Loss: 2.9033\n",
      "Epoch 7, Average Loss: 2.8259\n",
      "Epoch 8, Average Loss: 2.7649\n",
      "Epoch 9, Average Loss: 2.7149\n",
      "Epoch 10, Average Loss: 2.6736\n",
      "GRU Model - Training Time: 184.47s, Perplexity: 13.15\n"
     ]
    }
   ],
   "source": [
    "# Determine the device to be used (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Init models\n",
    "perceptron_model = PerceptronLanguageModel(st_vocab_size, embedding_dim, hidden_dim)\n",
    "rnn_model = RNNLanguageModel(st_vocab_size, embedding_dim, hidden_dim)\n",
    "st_lstm_model = LSTMLanguageModel(st_vocab_size, embedding_dim, hidden_dim)\n",
    "bpe_lstm_model = LSTMLanguageModel(bpe_vocab_size, embedding_dim, hidden_dim)\n",
    "gru_model = GRULanguageModel(st_vocab_size, embedding_dim, hidden_dim, num_layers=1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "perceptron_optimizer = optim.Adam(perceptron_model.parameters(), lr=learning_rate)\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "st_lstm_optimizer = optim.Adam(st_lstm_model.parameters(), lr=learning_rate)\n",
    "bpe_lstm_optimizer = optim.Adam(bpe_lstm_model.parameters(), lr=learning_rate)\n",
    "gru_optimizer = optim.Adam(gru_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train models\n",
    "print(\"Training perceptron model...\")\n",
    "perceptron_training_time = train_model_perceptron(perceptron_model, st_data_loader, criterion, perceptron_optimizer, num_epochs, device)\n",
    "perceptron_perplexity = calculate_perplexity(perceptron_model, st_data_loader, criterion, device, st_vocab_size)\n",
    "print(f\"Perceptron Model - Training Time: {perceptron_training_time:.2f}s, Perplexity: {perceptron_perplexity:.2f}\")\n",
    "\n",
    "print(\"Training RNN model...\")\n",
    "rnn_training_time = train_model_RNN(rnn_model, st_data_loader, criterion, rnn_optimizer, num_epochs, device)\n",
    "rnn_perplexity = calculate_perplexity(rnn_model, st_data_loader, criterion, device, st_vocab_size)\n",
    "print(f\"RNN Model - Training Time: {rnn_training_time:.2f}s, Perplexity: {rnn_perplexity:.2f}\")\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "st_lstm_training_time = train_model_st_lstm(st_lstm_model, st_data_loader, criterion, st_lstm_optimizer, num_epochs, device)\n",
    "st_lstm_perplexity = calculate_perplexity(st_lstm_model, st_data_loader, criterion, device, st_vocab_size)\n",
    "print(f\"LSTM Model - Training Time: {st_lstm_training_time:.2f}s, Perplexity: {st_lstm_perplexity:.2f}\")\n",
    "\n",
    "print(\"Training LSTM model with BPE Tokenizer...\")\n",
    "bpe_lstm_training_time = train_model_bpe_lstm(bpe_lstm_model, bpe_data_loader, criterion, bpe_lstm_optimizer, num_epochs, device)\n",
    "bpe_lstm_perplexity = calculate_perplexity(bpe_lstm_model, bpe_data_loader, criterion, device, bpe_vocab_size)\n",
    "print(f\"LSTM Model BPE Tokenizer - Training Time: {bpe_lstm_training_time:.2f}s, Perplexity: {bpe_lstm_perplexity:.2f}\")\n",
    "\n",
    "print(\"Training GRU model...\")\n",
    "gru_training_time= train_model_gru(gru_model, st_data_loader, criterion, gru_optimizer, num_epochs, device)\n",
    "gru_perplexity = calculate_perplexity(gru_model, st_data_loader, criterion, device, st_vocab_size)\n",
    "print(f\"GRU Model - Training Time: {gru_training_time:.2f}s, Perplexity: {gru_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron time:  152.98582983016968 Perceptron perplexity:  78.21942901611328\n",
      "RNN time:  172.80421090126038 RNN perplexity:  17.63882827758789\n",
      "LSTM time:  185.8695924282074 LSTM perplexity:  11.535888671875\n",
      "BPE LSTM time:  156.26988101005554 BPE LSTM perplexity:  13.371254920959473\n",
      "GRU time:  184.4695906639099 GRU perplexity:  13.15131664276123\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron time: \", perceptron_training_time, \"Perceptron perplexity: \", perceptron_perplexity)\n",
    "print(\"RNN time: \", rnn_training_time, \"RNN perplexity: \", rnn_perplexity)\n",
    "print(\"LSTM time: \", st_lstm_training_time, \"LSTM perplexity: \", st_lstm_perplexity)\n",
    "print(\"BPE LSTM time: \", bpe_lstm_training_time, \"BPE LSTM perplexity: \", bpe_lstm_perplexity)\n",
    "print(\"GRU time: \", gru_training_time, \"GRU perplexity: \", gru_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron: this will be good they are the fire and foe to the shire against the master he did well youve\n",
      "RNN: this will be the last disaster by the ground and its there were by many springs but they have seized\n",
      "LSTM: this will be the silent town and night came forward to him again and with him went landroval and meneldor\n",
      "LSTM BPE: this will be apart judge wreck inquisit judge ot studied bler leaps game preci pil ished attentively sway icing elrohir zing\n",
      "GRU: this will be laid to thought if he won it for a time he thought the mournful howling of wolves\n"
     ]
    }
   ],
   "source": [
    "start_sequence = ['this', 'will', 'be']\n",
    "\n",
    "\n",
    "generated_sentence_perceptron = generate_sentence_perceptron(perceptron_model, start_sequence, st_vocab, {v: k for k, v in st_vocab.items()})\n",
    "print(f\"Perceptron: {generated_sentence_perceptron}\")\n",
    "\n",
    "generated_sentence_RNN = generate_sentence_RNN(rnn_model, start_sequence, st_vocab, {v: k for k, v in st_vocab.items()})\n",
    "print(f\"RNN: {generated_sentence_RNN}\")\n",
    "\n",
    "generated_sentence_st_lstm = generate_sentence_st_lstm(st_lstm_model, start_sequence, st_vocab, {v: k for k, v in st_vocab.items()})\n",
    "print(f\"LSTM: {generated_sentence_st_lstm}\")\n",
    "\n",
    "generated_sentence_bpe_lstm = generate_sentence_bpe_lstm(bpe_lstm_model, start_sequence, tokenizer, bpe_vocab)\n",
    "print(f\"LSTM BPE: {generated_sentence_bpe_lstm}\")\n",
    "\n",
    "generated_sentence_gru = generate_sentence_gru(gru_model, start_sequence, st_vocab, {v: k for k, v in st_vocab.items()})\n",
    "print(f\"GRU: {generated_sentence_gru}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
